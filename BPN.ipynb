{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BPN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janakimeena/Python-for-Data-Science/blob/master/BPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhF334Ier2gA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-B81FxeyarH",
        "colab_type": "text"
      },
      "source": [
        "## **Activation Function**\n",
        "\n",
        "Function to introduce non-linearity\n",
        "\n",
        "Should be differentiable\n",
        "\n",
        "Monotonic function: A function which is either entirely non-increasing or non-decreasing.\n",
        "\n",
        "\n",
        "Activation Functions can be basically divided into 2 types\n",
        "1.   Linear Activation Function\n",
        "2.   Non-linear Activation Functions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbMqO7vozEED",
        "colab_type": "text"
      },
      "source": [
        "## Linear or Identity Activation Function\n",
        "\n",
        "Equation : f(x) = x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLvxP_qSzpo-",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid or Logistic Activation Function\n",
        "\n",
        "curve looks like a S-shape.\n",
        "\n",
        "exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivMqqh0rz8eP",
        "colab_type": "text"
      },
      "source": [
        " ## Softmax function \n",
        " \n",
        " More generalized logistic activation function which is used for multiclass classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68DiBcG0goM",
        "colab_type": "text"
      },
      "source": [
        "# Tanh or hyperbolic tangent Activation Function\n",
        "\n",
        "The range of the tanh function is from (-1 to 1). \n",
        "\n",
        "tanh is also sigmoidal (s - shaped).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qFHV_R70yoT",
        "colab_type": "text"
      },
      "source": [
        "## ReLU (Rectified Linear Unit) Activation Function\n",
        "\n",
        "Most used activation function in the world right now.Since, it is used in almost all the convolutional neural networks or deep learning.\n",
        "\n",
        "f(z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixYiQG1_sDRl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1+ np.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J_sHHEfsFfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Derivative of Sigmoid activation function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsXVCp9wT0DB",
        "colab_type": "code",
        "outputId": "4de63d10-eb65-4147-aa65-614aae012211",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image('../tmp/nn.jpg')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "../tmp/nn.jpg",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBrJ-4ZRVaxp",
        "colab_type": "text"
      },
      "source": [
        "# Rule for back propagating error\n",
        "\n",
        "Check how the change in the weight will make an influence in the loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfrdIcC1sNZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y):\n",
        "        self.input      = x\n",
        "        self.weights1   = np.random.rand(self.input.shape[1],4) \n",
        "        self.weights2   = np.random.rand(4,1)                 \n",
        "        self.y          = y\n",
        "        self.output     = np.zeros(self.y.shape)\n",
        "        print(\"Input Shape\",self.input.shape)\n",
        "        print(\"Weights_1 Shape\",self.weights1.shape)\n",
        "        print(\"Weights_2 Shape\",self.weights2.shape)\n",
        "        print(\"Output Shape\",self.output.shape)\n",
        "\n",
        "    def feedforward(self):\n",
        "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
        "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
        "\n",
        "    def backprop(self):\n",
        "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
        "        d_weights2 = - np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n",
        "        d_weights1 = - np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n",
        "\n",
        "        # update the weights with the derivative (slope) of the loss function\n",
        "        self.weights1 -= 0.1 * d_weights1\n",
        "        self.weights2 -= 0.1 * d_weights2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFqM8tKtsYS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array([\n",
        "[0, 0],\n",
        "[0, 1],\n",
        "[1, 0],\n",
        "[1, 1]\n",
        "])\n",
        "\n",
        "Y = np.array([\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1]\n",
        "])\n",
        "\n",
        "'''X = np.array([\n",
        "[0, 0],\n",
        "[0, 1],\n",
        "[1, 0],\n",
        "[1, 1]\n",
        "])\n",
        "\n",
        "Y = np.array([\n",
        "    [0],\n",
        "    [0],\n",
        "    [0],\n",
        "    [1]\n",
        "])'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOmKqf2dsk1f",
        "colab_type": "code",
        "outputId": "5d90057c-0cca-4434-ccf8-a5405827e161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "nn = NeuralNetwork(X,Y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Shape (4, 2)\n",
            "Weights_1 Shape (2, 4)\n",
            "Weights_2 Shape (4, 1)\n",
            "Output Shape (4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vItV9rlps2QT",
        "colab_type": "code",
        "outputId": "d7d01ca0-2191-4f46-f696-9ede06c81dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "for i in range(3000):\n",
        "    nn.feedforward()\n",
        "    nn.backprop()\n",
        "\n",
        "print(nn.output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.46334013e-04]\n",
            " [4.79324690e-01]\n",
            " [4.79545133e-01]\n",
            " [4.99652800e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9nq3phns5bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}